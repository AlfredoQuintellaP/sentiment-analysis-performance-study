{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztEQ3HA9HyKO"
      },
      "source": [
        "# [MI201] Sentiment Analysis - Comparative Study\n",
        "\n",
        "Comparative analysis of different ML approaches for sentiment analysis:\n",
        "1. Classical ML models\n",
        "2. BERT embeddings + MLP\n",
        "3. LLM-based classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The present machine learning work was jointly conducted by Alfredo Quintella, Helena Guachalla, and Naomy Gomes."
      ],
      "metadata": {
        "id": "P3568-AgAN_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGkycifmIKpm"
      },
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Classical ML models\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_auc_score, roc_curve,precision_recall_fscore_support, accuracy_score,\n",
        "                             confusion_matrix)\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Neural networks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import (BertTokenizer, BertModel,\n",
        "                         AutoTokenizer, AutoModel,\n",
        "                         get_linear_schedule_with_warmup)\n",
        "\n",
        "# Evaluation utilities\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "###eu add isso\n",
        "\n",
        "# Utility for progress bars(BERT)\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An3nBTfUIDMD"
      },
      "source": [
        "## Uploading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHmj5-nBG5mt"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download('abhi8923shriv/sentiment-analysis-dataset')\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n",
        "train_dataset = path + '/train.csv'\n",
        "test_dataset = path + '/test.csv'\n",
        "\n",
        "print(os.path.exists(train_dataset))\n",
        "print(os.path.exists(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50qgEODrWAxb"
      },
      "source": [
        "## Info of the Dataframes\n",
        "\n",
        "Fast look of the formats, types and quantity in the dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5MxhaSMWBFx"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_dataset, encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv(test_dataset, encoding='ISO-8859-1')\n",
        "\n",
        "print(f\"Train dataset shape: {train_df.shape}\")\n",
        "print(f\"Test dataset shape: {test_df.shape}\")\n",
        "\n",
        "print(f\"Training info: {train_df.info()}\")\n",
        "\n",
        "print(\"\\nFirst few rows of the training dataset:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nFirst few rows of the test dataset:\")\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_luWuahXgj0"
      },
      "source": [
        "Handling null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0DQflFMXgzZ"
      },
      "outputs": [],
      "source": [
        "train_df.isnull().sum()\n",
        "train_df = train_df.dropna()\n",
        "train_df.isnull().sum()\n",
        "\n",
        "test_df.isnull().sum()\n",
        "test_df = test_df.dropna()\n",
        "test_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-pvC6p4X_7n"
      },
      "source": [
        "## Removing stopwords & lowercasing all text\n",
        "\n",
        "Basically, the stopwords are words too common and that don't have that much of a semantic value. Some examples are: the, is, at, on, and.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1kTHKyfYAJi"
      },
      "outputs": [],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Text preprocessing function that removes stopwords and convert text to lowercase\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) #To remove characters that are not letters or numbers\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words and len(word)>1])\n",
        "\n",
        "    return text\n",
        "\n",
        "train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
        "test_df['processed_text'] = test_df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_EqsrHfY8H3"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITNoz_5PY8X0"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Sentiment distribution\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "train_counts = train_df['sentiment'].value_counts()\n",
        "axes[0].bar(train_counts.index, train_counts.values)\n",
        "axes[0].set_title('Train Sentiment Distribution')\n",
        "axes[0].set_xlabel('Sentiment')\n",
        "axes[0].set_ylabel('Count')\n",
        "for i, v in enumerate(train_counts.values):\n",
        "    axes[0].text(i, v + max(train_counts.values)*0.01, str(v), ha='center')\n",
        "\n",
        "test_counts = test_df['sentiment'].value_counts()\n",
        "axes[1].bar(test_counts.index, test_counts.values)\n",
        "axes[1].set_title('Test Sentiment Distribution')\n",
        "axes[1].set_xlabel('Sentiment')\n",
        "axes[1].set_ylabel('Count')\n",
        "for i, v in enumerate(test_counts.values):\n",
        "    axes[1].text(i, v + max(test_counts.values)*0.01, str(v), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za70GqQ5b_SG"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Text Length analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_df['text_length'] = train_df['processed_text'].apply(lambda x: len(x.split()))\n",
        "test_df['text_length'] = test_df['processed_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
        "sentiment_colors = {'negative': 'red', 'neutral': 'gray', 'positive': 'green'}\n",
        "\n",
        "\n",
        "axes[0].axvline(train_df['text_length'].mean(), color='red', linestyle='dashed',\n",
        "                linewidth=2, label=f'Mean: {train_df[\"text_length\"].mean():.1f}')\n",
        "axes[0].set_title('Train Text Length Distribution')\n",
        "axes[0].set_xlabel('Text Length')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "for sentiment in train_df['sentiment'].unique():\n",
        "    sentiment_data = train_df[train_df['sentiment'] == sentiment]\n",
        "    axes[0].hist(sentiment_data['text_length'], bins=50, color=sentiment_colors[sentiment],\n",
        "                 alpha=0.5, label=sentiment)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].axvline(test_df['text_length'].mean(), color='red', linestyle='dashed',\n",
        "                linewidth=2, label=f'Mean: {test_df[\"text_length\"].mean():.1f}')\n",
        "axes[1].set_title('Test Text Length Distribution')\n",
        "axes[1].set_xlabel('Text Length')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "for sentiment in test_df['sentiment'].unique():\n",
        "    sentiment_data = test_df[test_df['sentiment'] == sentiment]\n",
        "    axes[1].hist(sentiment_data['text_length'], bins=50, color=sentiment_colors[sentiment],\n",
        "                 alpha=0.5, label=sentiment)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBMPZ70PewVg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Word clouds\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
        "\n",
        "sentiments = ['negative', 'neutral', 'positive']\n",
        "titles = ['Negative', 'Neutral', 'Positive']\n",
        "colors = ['Reds', 'Grays', 'Greens']\n",
        "\n",
        "for idx, (sentiment, title, color) in enumerate(zip(sentiments, titles, colors)):\n",
        "    text = \" \".join(train_df[train_df['sentiment'] == sentiment]['processed_text'])\n",
        "    if text.strip():\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                              colormap=color, max_words=100).generate(text)\n",
        "        axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(title, fontsize=16)\n",
        "    else:\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# letter s possible doing nothing, check later why so common in every set and what it changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zstjqAMAloQx"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Analysing most frequent words\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_top_words(text_series, n=20):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    word_freq = Counter(all_words)\n",
        "    return word_freq.most_common(n)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
        "\n",
        "for idx, sentiment in enumerate(sentiments):\n",
        "    sentiment_text = train_df[train_df['sentiment'] == sentiment]['processed_text']\n",
        "    top_words = get_top_words(sentiment_text, 15)\n",
        "\n",
        "    words, counts = zip(*top_words) if top_words else ([], [])\n",
        "\n",
        "    if words:\n",
        "        axes[idx].barh(range(len(words)), counts, color=sentiment_colors[sentiment])\n",
        "        axes[idx].set_yticks(range(len(words)))\n",
        "        axes[idx].set_yticklabels(words)\n",
        "        axes[idx].invert_yaxis()\n",
        "        axes[idx].set_title(f'Top 15 Words for {sentiment.capitalize()} Sentiment', fontsize=16)\n",
        "        axes[idx].set_xlabel('Frequency')\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, 'No words found',\n",
        "                       ha = 'center', va='center')\n",
        "        axes[idx].set_title(f'Top 15 Words for {sentiment.capitalize()} Sentiment', fontsize=16)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbPqbSscprc7"
      },
      "source": [
        "# Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31wM37bFp-I9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Encoding sentiment labels\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['encoded_sentiment'] = label_encoder.fit_transform(train_df['sentiment'])\n",
        "test_df['encoded_sentiment'] = label_encoder.transform(test_df['sentiment'])\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Preparing data splitss\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train_classical = train_df['processed_text']\n",
        "y_train_classical = train_df['encoded_sentiment']\n",
        "X_test_classical = test_df['processed_text']\n",
        "y_test_classical = test_df['encoded_sentiment']\n",
        "\n",
        "# For validation splis (for neural networks)\n",
        "X_temp, X_val, y_temp, y_val = train_test_split(X_train_classical, y_train_classical,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_temp)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test_classical)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Organizing vectorization options\")\n",
        "print(\"=\"*60)\n",
        "# prints the same values over and over (hopefully), so we can check if any vectorization\n",
        "# made a mstk\n",
        "\n",
        "print(\"TF-IDF Vectorization:\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,              #\n",
        "    ngram_range=(1, 2),             # using unigrams and bigrams\n",
        "    stop_words='english',\n",
        "    min_df=5,                       # ignoring terms with freq less than 5\n",
        "    max_df=0.7,                     # ignoring terms with freq bigger than 70%\n",
        "    sublinear_tf=True               # using sublinear scaling\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_temp)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_classical)\n",
        "print(f\"TF-IDF Train set shape: {X_train_tfidf.shape}\")\n",
        "print(f\"TF-IDF Validation set shape: {X_val_tfidf.shape}\")\n",
        "print(f\"TF-IDF Test set shape: {X_test_tfidf.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Count vectorization (Bag of Words):\")\n",
        "\n",
        "count_vectorizer = CountVectorizer(\n",
        "    max_features=5000,              #\n",
        "    ngram_range=(1, 2),             # using unigrams and bigrams\n",
        "    stop_words='english',\n",
        "    min_df=5,                       # ignoring terms with freq less than 5\n",
        "    max_df=0.7                      # ignoring terms with freq bigger than 70%\n",
        ")\n",
        "\n",
        "X_train_count = count_vectorizer.fit_transform(X_temp)\n",
        "X_val_count = count_vectorizer.transform(X_val)\n",
        "X_test_count = count_vectorizer.transform(X_test_classical)\n",
        "print(f\"Count Train set shape: {X_train_count.shape}\")\n",
        "print(f\"Count Validation set shape: {X_val_count.shape}\")\n",
        "print(f\"Count Test set shape: {X_test_count.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nBERT tokenizer:\")\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(f\"BERT tokenizer: {bert_tokenizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgnPepZ7uNAP"
      },
      "source": [
        "## Now we can start answering the questions\n",
        "\n",
        "First, lets remember all the questions:\n",
        "\n",
        "- Question 0: please analyse the dataset with differents classical machine learning model\n",
        "- Question 1: please perform a classification with differents classical machine learning model and analyse the performences\n",
        "- Question 2: please perform a classification with a MLP?\n",
        "- Question 3: please analyse all the performences and explain which is the best\n",
        "- Question 4: please use an LLM compare your performences to a LLM\n",
        "- Quesiton 5: please explain why I choose a BERT embedding instead of the raw text\n",
        "- Question 6: please read the BERT paper and explain the BERT architecture\n",
        "- Question 7: please finetue with LORA an LLM to classify the sentiment (optional)\n",
        "\n",
        "To organize everything, the sections we re going to make are:\n",
        "\n",
        "### Section 1: Classical ML Models:\n",
        "1. Naive Bayes Classifier\n",
        "2. Logistic Regression\n",
        "3. SVM classifier\n",
        "4. Random Forest\n",
        "5. XGBoost\n",
        "6. Model Comparision Table and Theoric Conclusion\n",
        "### Section 2: Neural Network aproaches:\n",
        "1. MLP Classifier\n",
        "2. BERT Embeddings + Classifier\n",
        "3. BERT Fine-tuning\n",
        "4. Perfomance Comparisions and Theoric Conclusion\n",
        "### Section 3: LLM-Based Classification\n",
        "1. Zero-shot Classification with GPT/LLaMA\n",
        "2. Few-shot/Prompt Engineering\n",
        "3. Comparison with previous models\n",
        "### Section 4: Comprehensive Analysis & Conclusions\n",
        "1. All Models Perfomance Comparison Table\n",
        "2. Error Analysis (confusions matrices, misclassified samples)\n",
        "3. Computational Cost Analysis (training/inference time)\n",
        "4. Why BERT embedding?\n",
        "5. BERT Architecture Explaination\n",
        "### Section 5: Answers Summary & References\n",
        "1. Summary Table with All Question Answers\n",
        "2. General conclusion and bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI0PwRZDqShD"
      },
      "source": [
        "# Section 1: Classical ML Models:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store all results\n",
        "classical_results = {}\n",
        "\n",
        "def train_and_evaluate(model, model_name, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train a model and evaluate its performance\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Training\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Prediction\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"Training Time: {train_time:.2f}s\")\n",
        "    print(f\"Inference Time: {inference_time:.4f}s\")\n",
        "\n",
        "    # Store results\n",
        "    classical_results[model_name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'train_time': train_time,\n",
        "        'inference_time': inference_time,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "    return model, y_pred\n"
      ],
      "metadata": {
        "id": "yneJPSNeyOrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Naive Bayes Classifier\n"
      ],
      "metadata": {
        "id": "ISBQQCrPycSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1. MULTINOMIAL NAIVE BAYES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. MULTINOMIAL NAIVE BAYES CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "print(\"Theory: Assumes feature independence and uses word frequency.\")\n",
        "print(\"Best for: Text classification with count-based features.\")\n",
        "\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "train_and_evaluate(nb_model, \"Multinomial Naive Bayes\",\n",
        "                   X_train_count, y_temp, X_test_count, y_test_classical)\n"
      ],
      "metadata": {
        "id": "lkGfSW46y7pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Logistic Regression"
      ],
      "metadata": {
        "id": "2s7B1RT5yc0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. LOGISTIC REGRESSION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"Theory: Linear model with sigmoid activation for classification.\")\n",
        "print(\"Best for: Fast baseline with good interpretability.\")\n",
        "\n",
        "lr_model = LogisticRegression(\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    solver='saga'\n",
        ")\n",
        "train_and_evaluate(lr_model, \"Logistic Regression\",\n",
        "                   X_train_tfidf, y_temp, X_test_tfidf, y_test_classical)\n"
      ],
      "metadata": {
        "id": "u27HPEkty-Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. SVM classifier"
      ],
      "metadata": {
        "id": "BTdOh0kays0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. LINEAR SVM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. LINEAR SUPPORT VECTOR MACHINE\")\n",
        "print(\"=\"*80)\n",
        "print(\"Theory: Finds optimal hyperplane to separate classes.\")\n",
        "print(\"Best for: High-dimensional sparse data like text.\")\n",
        "\n",
        "svm_model = LinearSVC(\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "train_and_evaluate(svm_model, \"Linear SVM\",\n",
        "                   X_train_tfidf, y_temp, X_test_tfidf, y_test_classical)\n"
      ],
      "metadata": {
        "id": "WU1XuKZrzBBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Random Forest"
      ],
      "metadata": {
        "id": "D6Icxglfywn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. RANDOM FOREST\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. RANDOM FOREST CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "print(\"Theory: Ensemble of decision trees with majority voting.\")\n",
        "print(\"Best for: Capturing non-linear patterns, robust to overfitting.\")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=50,\n",
        "    min_samples_split=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "train_and_evaluate(rf_model, \"Random Forest\",\n",
        "                   X_train_tfidf, y_temp, X_test_tfidf, y_test_classical)\n"
      ],
      "metadata": {
        "id": "8Z6POdWLzDPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. XGBoost"
      ],
      "metadata": {
        "id": "kqu3DTOwy1XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. XGBOOST\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5. XGBOOST CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "print(\"Theory: Gradient boosting with regularization and efficient implementation.\")\n",
        "print(\"Best for: High performance on structured/semi-structured data.\")\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "train_and_evaluate(xgb_model, \"XGBoost\",\n",
        "                   X_train_tfidf, y_temp, X_test_tfidf, y_test_classical)\n"
      ],
      "metadata": {
        "id": "y7QBqXQozF9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6. Model Comparision Table and Theoric Conclusion."
      ],
      "metadata": {
        "id": "CIfrLDIWy2qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6. MODEL COMPARISON TABLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSICAL ML MODELS - PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(classical_results.keys()),\n",
        "    'Accuracy': [v['accuracy'] for v in classical_results.values()],\n",
        "    'Precision': [v['precision'] for v in classical_results.values()],\n",
        "    'Recall': [v['recall'] for v in classical_results.values()],\n",
        "    'F1-Score': [v['f1'] for v in classical_results.values()],\n",
        "    'Train Time (s)': [v['train_time'] for v in classical_results.values()],\n",
        "    'Inference Time (s)': [v['inference_time'] for v in classical_results.values()]\n",
        "})\n",
        "\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0, 0].barh(comparison_df['Model'], comparison_df['Accuracy'])\n",
        "axes[0, 0].set_xlabel('Accuracy')\n",
        "axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "axes[0, 0].set_xlim([0, 1])\n",
        "\n",
        "# F1-Score comparison\n",
        "axes[0, 1].barh(comparison_df['Model'], comparison_df['F1-Score'], color='green')\n",
        "axes[0, 1].set_xlabel('F1-Score')\n",
        "axes[0, 1].set_title('Model F1-Score Comparison')\n",
        "axes[0, 1].set_xlim([0, 1])\n",
        "\n",
        "# Training time\n",
        "axes[1, 0].barh(comparison_df['Model'], comparison_df['Train Time (s)'], color='orange')\n",
        "axes[1, 0].set_xlabel('Training Time (seconds)')\n",
        "axes[1, 0].set_title('Training Time Comparison')\n",
        "\n",
        "# Metrics radar chart\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "best_model = comparison_df.iloc[0]['Model']\n",
        "best_values = [\n",
        "    classical_results[best_model]['accuracy'],\n",
        "    classical_results[best_model]['precision'],\n",
        "    classical_results[best_model]['recall'],\n",
        "    classical_results[best_model]['f1']\n",
        "]\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "best_values += best_values[:1]\n",
        "angles += angles[:1]\n",
        "\n",
        "axes[1, 1].plot(angles, best_values, 'o-', linewidth=2, label=best_model)\n",
        "axes[1, 1].fill(angles, best_values, alpha=0.25)\n",
        "axes[1, 1].set_xticks(angles[:-1])\n",
        "axes[1, 1].set_xticklabels(metrics)\n",
        "axes[1, 1].set_ylim(0, 1)\n",
        "axes[1, 1].set_title(f'Best Model ({best_model}) - Performance Radar')\n",
        "axes[1, 1].grid(True)\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"THEORETICAL CONCLUSIONS - CLASSICAL ML MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
        "\n",
        "print(f\"\"\"\n",
        "BEST PERFORMING MODEL: {best_model_name} (F1-Score: {best_f1:.4f})\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "DIoqKi2-zLog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KEY OBSERVATIONS:\n",
        "\n",
        "1. LINEAR MODELS (Logistic Regression, SVM):\n",
        "   - Fast training and inference\n",
        "   - Work well with TF-IDF features\n",
        "   - Good interpretability\n",
        "   - Scale well to large datasets\n",
        "\n",
        "2. ENSEMBLE METHODS (Random Forest, XGBoost):\n",
        "   - Can capture non-linear patterns\n",
        "   - More complex and slower to train\n",
        "   - May overfit on text data without proper tuning\n",
        "   - Good for feature interactions\n",
        "\n",
        "3. NAIVE BAYES:\n",
        "   - Extremely fast training\n",
        "   - Works with count vectorization\n",
        "   - Strong independence assumption may limit performance\n",
        "   - Good baseline model\n",
        "\n",
        "4. RECOMMENDATION:\n",
        "   For production sentiment analysis with classical ML, we recommend:\n",
        "   - Logistic Regression for fast inference and interpretability\n",
        "   - Naive Bayes for resource-constrained environments\n",
        "\n",
        "5. LIMITATIONS OF CLASSICAL ML:\n",
        "   - Cannot capture long-range dependencies\n",
        "   - Limited context understanding\n",
        "   - Manual feature engineering required\n",
        "   - No semantic understanding of words\n",
        "\n",
        "These limitations motivate the use of neural network approaches in Section 2"
      ],
      "metadata": {
        "id": "FFqKUxxxzjW5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erV8toCvtl-l"
      },
      "source": [
        "# Section 2: Neural Network aproaches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQQbx6DMyKmz"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dictionary to store neural network results\n",
        "nn_results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiWTX7e9yOSa"
      },
      "source": [
        "1. MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tUQsDk3yrRi"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. MLP CLASSIFIER WITH TF-IDF FEATURES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. MULTI-LAYER PERCEPTRON (MLP) CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], num_classes=3, dropout=0.3):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Prepare data for MLP\n",
        "X_train_mlp = torch.FloatTensor(X_train_tfidf.toarray()).to(device)\n",
        "y_train_mlp = torch.LongTensor(y_temp.values).to(device)\n",
        "X_val_mlp = torch.FloatTensor(X_val_tfidf.toarray()).to(device)\n",
        "y_val_mlp = torch.LongTensor(y_val.values).to(device)\n",
        "X_test_mlp = torch.FloatTensor(X_test_tfidf.toarray()).to(device)\n",
        "y_test_mlp = torch.LongTensor(y_test_classical.values).to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_mlp, y_train_mlp)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)#!!!!!!!!Aumentei o batch para ir mais rapido\n",
        "\n",
        "# Initialize model\n",
        "mlp_model = MLPClassifier(input_dim=X_train_tfidf.shape[1], num_classes=3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Training\n",
        "print(\"Training MLP...\")\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    mlp_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = mlp_model(X_val_mlp)\n",
        "        val_preds = torch.argmax(val_outputs, dim=1)\n",
        "        val_acc = (val_preds == y_val_mlp).float().mean().item()\n",
        "\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss/len(train_loader):.4f} - Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "mlp_train_time = time.time() - start_time\n",
        "\n",
        "# Test evaluation\n",
        "mlp_model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    test_outputs = mlp_model(X_test_mlp)\n",
        "    mlp_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
        "mlp_inference_time = time.time() - start_time\n",
        "\n",
        "# Calculate metrics\n",
        "mlp_accuracy = accuracy_score(y_test_classical, mlp_predictions)\n",
        "mlp_precision, mlp_recall, mlp_f1, _ = precision_recall_fscore_support(\n",
        "    y_test_classical, mlp_predictions, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\nMLP Results:\")\n",
        "print(f\"Accuracy: {mlp_accuracy:.4f}\")\n",
        "print(f\"Precision: {mlp_precision:.4f}\")\n",
        "print(f\"Recall: {mlp_recall:.4f}\")\n",
        "print(f\"F1-Score: {mlp_f1:.4f}\")\n",
        "print(f\"Training Time: {mlp_train_time:.2f}s\")\n",
        "print(f\"Inference Time: {mlp_inference_time:.4f}s\")\n",
        "\n",
        "nn_results['MLP (TF-IDF)'] = {\n",
        "    'accuracy': mlp_accuracy,\n",
        "    'precision': mlp_precision,\n",
        "    'recall': mlp_recall,\n",
        "    'f1': mlp_f1,\n",
        "    'train_time': mlp_train_time,\n",
        "    'inference_time': mlp_inference_time,\n",
        "    'predictions': mlp_predictions\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2yUsCduyaEE"
      },
      "source": [
        "2. BERT Embeddings + Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRckbzBnzAXa"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. BERT EMBEDDINGS + CLASSIFIER (OPTIMIZED - EXTRACT ONCE, TRAIN FAST)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. BERT EMBEDDINGS + MLP CLASSIFIER (OPTIMIZED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_BATCH_SIZE = 128  # Larger batch for embedding extraction\n",
        "\n",
        "# Helper function to extract embeddings efficiently\n",
        "def extract_bert_embeddings(texts, tokenizer, model, device, batch_size=128, max_length=128):\n",
        "    \"\"\"\n",
        "    Extract BERT embeddings in batches (ONE TIME ONLY).\n",
        "    This is the slow part, so we do it once and cache the results.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Convert to list if needed\n",
        "    if hasattr(texts, 'values'):\n",
        "        texts = texts.values\n",
        "\n",
        "    num_samples = len(texts)\n",
        "\n",
        "    print(f\"Extracting embeddings for {num_samples} samples...\")\n",
        "    print(f\"Batch size: {batch_size}, Max length: {max_length}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            # Progress indicator\n",
        "            if i % (batch_size * 10) == 0:\n",
        "                progress = (i / num_samples) * 100\n",
        "                elapsed = time.time() - start_time\n",
        "                eta = (elapsed / (i + 1)) * (num_samples - i) if i > 0 else 0\n",
        "                print(f\"  Progress: {progress:.1f}% ({i}/{num_samples}) - Elapsed: {elapsed:.1f}s - ETA: {eta:.1f}s\")\n",
        "\n",
        "            # Get batch\n",
        "            batch_end = min(i + batch_size, num_samples)\n",
        "            batch_texts = [str(text) for text in texts[i:batch_end]]\n",
        "\n",
        "            # Tokenize\n",
        "            encodings = tokenizer(\n",
        "                batch_texts,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = encodings['input_ids'].to(device)\n",
        "            attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "            # Extract embeddings\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            embeddings = outputs.pooler_output  # [CLS] token representation\n",
        "\n",
        "            # Move to CPU and append\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    final_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"  Extraction complete in {total_time:.2f}s\")\n",
        "    print(f\"  Embeddings shape: {final_embeddings.shape}\")\n",
        "\n",
        "    return final_embeddings\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "print(\"\\nLoading BERT model...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "# Freeze BERT (we're only using it for feature extraction)\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# EXTRACT EMBEDDINGS ONCE (THIS IS THE SLOW PART - BUT ONLY ONCE!)\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"EXTRACTING BERT EMBEDDINGS (ONE TIME OPERATION)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "extraction_start = time.time()\n",
        "\n",
        "# Extract for all datasets\n",
        "train_embeddings = extract_bert_embeddings(\n",
        "    X_temp, bert_tokenizer, bert_model, device,\n",
        "    batch_size=EMBEDDING_BATCH_SIZE, max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "val_embeddings = extract_bert_embeddings(\n",
        "    X_val, bert_tokenizer, bert_model, device,\n",
        "    batch_size=EMBEDDING_BATCH_SIZE, max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "test_embeddings = extract_bert_embeddings(\n",
        "    X_test_classical, bert_tokenizer, bert_model, device,\n",
        "    batch_size=EMBEDDING_BATCH_SIZE, max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "total_extraction_time = time.time() - extraction_start\n",
        "\n",
        "print(f\"\\n  All embeddings extracted in {total_extraction_time:.2f}s\")\n",
        "print(f\"  Train: {train_embeddings.shape}\")\n",
        "print(f\"  Val: {val_embeddings.shape}\")\n",
        "print(f\"  Test: {test_embeddings.shape}\")\n",
        "\n",
        "# Free GPU memory\n",
        "del bert_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Dataset for pre-computed embeddings\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = torch.LongTensor(labels.values if hasattr(labels, 'values') else labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "# Create datasets\n",
        "train_emb_dataset = EmbeddingDataset(train_embeddings, y_temp)\n",
        "val_emb_dataset = EmbeddingDataset(val_embeddings, y_val)\n",
        "test_emb_dataset = EmbeddingDataset(test_embeddings, y_test_classical)\n",
        "\n",
        "# Create dataloaders\n",
        "train_emb_loader = DataLoader(train_emb_dataset, batch_size=256, shuffle=True)\n",
        "val_emb_loader = DataLoader(val_emb_dataset, batch_size=512, shuffle=False)\n",
        "test_emb_loader = DataLoader(test_emb_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Simple classifier on top of BERT embeddings\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=3, dropout=0.3):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Initialize classifier\n",
        "bert_classifier = BERTClassifier(num_classes=3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(bert_classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Training (FAST because embeddings are pre-computed!)\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"TRAINING CLASSIFIER ON PRE-COMPUTED EMBEDDINGS (FAST)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "num_epochs = 15\n",
        "best_val_acc = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "train_losses_bert = []\n",
        "val_accuracies_bert = []\n",
        "\n",
        "train_start = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    bert_classifier.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for embeddings, labels in train_emb_loader:\n",
        "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = bert_classifier(embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    bert_classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_emb_loader:\n",
        "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "            outputs = bert_classifier(embeddings)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = correct / total\n",
        "    avg_loss = epoch_loss / len(train_emb_loader)\n",
        "\n",
        "    train_losses_bert.append(avg_loss)\n",
        "    val_accuracies_bert.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(bert_classifier.state_dict(), 'best_bert_classifier.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "bert_train_time = time.time() - train_start\n",
        "\n",
        "# Load best model and evaluate\n",
        "bert_classifier.load_state_dict(torch.load('best_bert_classifier.pt'))\n",
        "bert_classifier.eval()\n",
        "\n",
        "# Test predictions\n",
        "bert_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "inference_start = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_emb_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        outputs = bert_classifier(embeddings)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        bert_predictions.extend(predicted.cpu().numpy())\n",
        "        test_labels_list.extend(labels.numpy())\n",
        "\n",
        "bert_inference_time = time.time() - inference_start\n",
        "\n",
        "# Calculate metrics\n",
        "bert_accuracy = accuracy_score(test_labels_list, bert_predictions)\n",
        "bert_precision, bert_recall, bert_f1, _ = precision_recall_fscore_support(\n",
        "    test_labels_list, bert_predictions, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"BERT EMBEDDINGS + CLASSIFIER RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Accuracy:  {bert_accuracy:.4f}\")\n",
        "print(f\"Precision: {bert_precision:.4f}\")\n",
        "print(f\"Recall:    {bert_recall:.4f}\")\n",
        "print(f\"F1-Score:  {bert_f1:.4f}\")\n",
        "print(f\"\\nTIME BREAKDOWN:\")\n",
        "print(f\"  Embedding extraction: {total_extraction_time:.2f}s (one-time cost)\")\n",
        "print(f\"  Classifier training:  {bert_train_time:.2f}s\")\n",
        "print(f\"  Inference time:       {bert_inference_time:.4f}s\")\n",
        "print(f\"  Total time:           {total_extraction_time + bert_train_time:.2f}s\")\n",
        "\n",
        "# Save results\n",
        "nn_results['BERT Embeddings + Classifier'] = {\n",
        "    'accuracy': bert_accuracy,\n",
        "    'precision': bert_precision,\n",
        "    'recall': bert_recall,\n",
        "    'f1': bert_f1,\n",
        "    'train_time': bert_train_time,\n",
        "    'inference_time': bert_inference_time,\n",
        "    'extraction_time': total_extraction_time,\n",
        "    'predictions': bert_predictions\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEsyBq27yelt"
      },
      "source": [
        "3. BERT Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZRbGaBGzMUu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# SECTION 2.3: BERT FINE-TUNING (OPTIMIZED VERSION)\n",
        "# Complete BERT fine-tuning with all parameters trainable\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 3. BERT FINE-TUNING (COMPLETE MODEL - OPTIONAL BUT BEST PERFORMANCE)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. BERT FINE-TUNING (Complete Model Training)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Note: This is slower but achieves the best performance\")\n",
        "print(\"Updating all 110M BERT parameters + classifier\")\n",
        "\n",
        "# Clean GPU memory before starting\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Dataset class for BERT (with tokenization on-the-fly)\n",
        "class BERTTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts.values if hasattr(texts, 'values') else texts\n",
        "        self.labels = labels.values if hasattr(labels, 'values') else labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize on-the-fly\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Fine-tuned BERT model\n",
        "class BERTFineTuned(nn.Module):\n",
        "    \"\"\"Complete BERT with trainable parameters\"\"\"\n",
        "    def __init__(self, num_classes=3, dropout=0.1):\n",
        "        super(BERTFineTuned, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT forward pass (trainable)\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use [CLS] token representation\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Classifier\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Hyperparameters optimized for fine-tuning\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 16  # Smaller batch for fine-tuning (memory constraints)\n",
        "LEARNING_RATE = 2e-5  # Small LR for fine-tuning pre-trained model\n",
        "NUM_EPOCHS = 3  # Typically 2-4 epochs for BERT fine-tuning\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nPreparing datasets for fine-tuning...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_ft_dataset = BERTTextDataset(X_temp, y_temp, bert_tokenizer, MAX_LENGTH)\n",
        "val_ft_dataset = BERTTextDataset(X_val, y_val, bert_tokenizer, MAX_LENGTH)\n",
        "test_ft_dataset = BERTTextDataset(X_test_classical, y_test_classical, bert_tokenizer, MAX_LENGTH)\n",
        "\n",
        "# Create dataloaders\n",
        "train_ft_loader = DataLoader(train_ft_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_ft_loader = DataLoader(val_ft_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
        "test_ft_loader = DataLoader(test_ft_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"  Train: {len(train_ft_dataset)} samples ({len(train_ft_loader)} batches)\")\n",
        "print(f\"  Val: {len(val_ft_dataset)} samples ({len(val_ft_loader)} batches)\")\n",
        "print(f\"  Test: {len(test_ft_dataset)} samples ({len(test_ft_loader)} batches)\")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\nInitializing BERT model for fine-tuning...\")\n",
        "bert_finetuned = BERTFineTuned(num_classes=3, dropout=0.1).to(device)\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in bert_finetuned.parameters())\n",
        "trainable_params = sum(p.numel() for p in bert_finetuned.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Optimizer with weight decay (AdamW recommended for BERT)\n",
        "optimizer = optim.AdamW(\n",
        "    bert_finetuned.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    eps=1e-8,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "total_steps = len(train_ft_loader) * NUM_EPOCHS\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Total training steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (important for BERT)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Progress indicator\n",
        "        if (batch_idx + 1) % max(1, len(dataloader) // 5) == 0:\n",
        "            progress = (batch_idx + 1) / len(dataloader) * 100\n",
        "            current_loss = total_loss / (batch_idx + 1)\n",
        "            current_acc = correct / total if total > 0 else 0\n",
        "            print(f\"  Batch {batch_idx+1}/{len(dataloader)} ({progress:.1f}%) - \"\n",
        "                  f\"Loss: {current_loss:.4f} - Acc: {current_acc:.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy, all_preds, all_labels\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING BERT FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_val_accuracy = 0\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "training_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Training\n",
        "    print(\"Training phase:\")\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        bert_finetuned, train_ft_loader, optimizer, criterion, scheduler, device\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\nValidation phase:\")\n",
        "    val_loss, val_acc, _, _ = validate(\n",
        "        bert_finetuned, val_ft_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_accuracy:\n",
        "        best_val_accuracy = val_acc\n",
        "        torch.save(bert_finetuned.state_dict(), 'best_bert_finetuned.pt')\n",
        "        print(f\"    Best model saved! (Val Acc: {val_acc:.4f})\")\n",
        "\n",
        "bert_finetune_train_time = time.time() - training_start\n",
        "\n",
        "print(f\"\\n  Fine-tuning completed in {bert_finetune_train_time:.2f}s\")\n",
        "print(f\"  Best validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "# Load best model for testing\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bert_finetuned.load_state_dict(torch.load('best_bert_finetuned.pt'))\n",
        "bert_finetuned.eval()\n",
        "\n",
        "# Test predictions\n",
        "test_preds = []\n",
        "test_labels_list = []\n",
        "\n",
        "inference_start = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_ft_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = bert_finetuned(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels_list.extend(labels.numpy())\n",
        "\n",
        "        # Progress\n",
        "        if (batch_idx + 1) % max(1, len(test_ft_loader) // 4) == 0:\n",
        "            progress = (batch_idx + 1) / len(test_ft_loader) * 100\n",
        "            print(f\"  Test batch {batch_idx+1}/{len(test_ft_loader)} ({progress:.1f}%)\")\n",
        "\n",
        "bert_finetune_inference_time = time.time() - inference_start\n",
        "\n",
        "# Calculate metrics\n",
        "bert_ft_accuracy = accuracy_score(test_labels_list, test_preds)\n",
        "bert_ft_precision, bert_ft_recall, bert_ft_f1, _ = precision_recall_fscore_support(\n",
        "    test_labels_list, test_preds, average='weighted'\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"BERT FINE-TUNING - FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Accuracy:  {bert_ft_accuracy:.4f}\")\n",
        "print(f\"Precision: {bert_ft_precision:.4f}\")\n",
        "print(f\"Recall:    {bert_ft_recall:.4f}\")\n",
        "print(f\"F1-Score:  {bert_ft_f1:.4f}\")\n",
        "print(f\"\\nPERFORMANCE METRICS:\")\n",
        "print(f\"  Training time:   {bert_finetune_train_time:.2f}s ({bert_finetune_train_time/60:.2f} min)\")\n",
        "print(f\"  Inference time:  {bert_finetune_inference_time:.4f}s\")\n",
        "print(f\"  Epochs trained:  {len(train_losses)}\")\n",
        "\n",
        "# Store results\n",
        "nn_results['BERT Fine-tuned'] = {\n",
        "    'accuracy': bert_ft_accuracy,\n",
        "    'precision': bert_ft_precision,\n",
        "    'recall': bert_ft_recall,\n",
        "    'f1': bert_ft_f1,\n",
        "    'train_time': bert_finetune_train_time,\n",
        "    'inference_time': bert_finetune_inference_time,\n",
        "    'predictions': test_preds\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "print(\"\\nGenerating training visualizations...\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Training loss\n",
        "axes[0, 0].plot(train_losses, marker='o', label='Train Loss', linewidth=2)\n",
        "axes[0, 0].plot(val_losses, marker='s', label='Val Loss', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('BERT Fine-tuning - Loss Curves')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training accuracy\n",
        "axes[0, 1].plot(train_accuracies, marker='o', label='Train Acc', linewidth=2)\n",
        "axes[0, 1].plot(val_accuracies, marker='s', label='Val Acc', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('BERT Fine-tuning - Accuracy Curves')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels_list, test_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "axes[1, 0].set_title('Confusion Matrix - BERT Fine-tuned')\n",
        "axes[1, 0].set_ylabel('True Label')\n",
        "axes[1, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "# Compare all neural network approaches\n",
        "nn_models = list(nn_results.keys())\n",
        "nn_f1_scores = [nn_results[model]['f1'] for model in nn_models]\n",
        "\n",
        "axes[1, 1].barh(nn_models, nn_f1_scores, color=['skyblue', 'lightgreen', 'coral'])\n",
        "axes[1, 1].set_xlabel('F1-Score')\n",
        "axes[1, 1].set_title('Neural Network Models - F1-Score Comparison')\n",
        "axes[1, 1].set_xlim([0, 1])\n",
        "for i, (model, score) in enumerate(zip(nn_models, nn_f1_scores)):\n",
        "    axes[1, 1].text(score + 0.01, i, f'{score:.4f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('bert_finetuning_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n  Visualizations saved as 'bert_finetuning_results.png'\")\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE NEURAL NETWORK COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_summary = pd.DataFrame({\n",
        "    'Model': nn_models,\n",
        "    'F1-Score': nn_f1_scores,\n",
        "    'Accuracy': [nn_results[m]['accuracy'] for m in nn_models],\n",
        "    'Train Time (s)': [nn_results[m]['train_time'] for m in nn_models],\n",
        "    'Inference Time (s)': [nn_results[m]['inference_time'] for m in nn_models]\n",
        "})\n",
        "\n",
        "comparison_summary = comparison_summary.sort_values('F1-Score', ascending=False)\n",
        "print(comparison_summary.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fdL9vuTyhlu"
      },
      "source": [
        "4. Perfomance Comparisions and Theoric Conclusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVcaT4UXz-Bd"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. NEURAL NETWORK PERFORMANCE COMPARISON & THEORETICAL CONCLUSIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. NEURAL NETWORK MODELS - PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "nn_comparison_df = pd.DataFrame({\n",
        "    'Model': list(nn_results.keys()),\n",
        "    'Accuracy': [v['accuracy'] for v in nn_results.values()],\n",
        "    'Precision': [v['precision'] for v in nn_results.values()],\n",
        "    'Recall': [v['recall'] for v in nn_results.values()],\n",
        "    'F1-Score': [v['f1'] for v in nn_results.values()],\n",
        "    'Train Time (s)': [v['train_time'] for v in nn_results.values()],\n",
        "    'Inference Time (s)': [v['inference_time'] for v in nn_results.values()]\n",
        "})\n",
        "\n",
        "print(nn_comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0, 0].bar(nn_results.keys(), [v['accuracy'] for v in nn_results.values()])\n",
        "axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate([v['accuracy'] for v in nn_results.values()]):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# F1-Score comparison\n",
        "axes[0, 1].bar(nn_results.keys(), [v['f1'] for v in nn_results.values()])\n",
        "axes[0, 1].set_title('Model F1-Score Comparison')\n",
        "axes[0, 1].set_ylabel('F1-Score')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate([v['f1'] for v in nn_results.values()]):\n",
        "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# Training time comparison\n",
        "axes[0, 2].bar(nn_results.keys(), [v['train_time'] for v in nn_results.values()])\n",
        "axes[0, 2].set_title('Training Time Comparison')\n",
        "axes[0, 2].set_ylabel('Time (seconds)')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate([v['train_time'] for v in nn_results.values()]):\n",
        "    axes[0, 2].text(i, v + max([v['train_time'] for v in nn_results.values()])*0.01,\n",
        "                    f'{v:.0f}s', ha='center')\n",
        "\n",
        "# Inference time comparison\n",
        "axes[1, 0].bar(nn_results.keys(), [v['inference_time'] for v in nn_results.values()])\n",
        "axes[1, 0].set_title('Inference Time Comparison')\n",
        "axes[1, 0].set_ylabel('Time (seconds)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate([v['inference_time'] for v in nn_results.values()]):\n",
        "    axes[1, 0].text(i, v + max([v['inference_time'] for v in nn_results.values()])*0.01,\n",
        "                    f'{v:.2f}s', ha='center')\n",
        "\n",
        "# Precision-Recall comparison\n",
        "x = np.arange(len(nn_results))\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x - width/2, [v['precision'] for v in nn_results.values()], width, label='Precision')\n",
        "axes[1, 1].bar(x + width/2, [v['recall'] for v in nn_results.values()], width, label='Recall')\n",
        "axes[1, 1].set_title('Precision vs Recall Comparison')\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(nn_results.keys(), rotation=45)\n",
        "axes[1, 1].legend()\n",
        "\n",
        "# Performance vs Time scatter\n",
        "axes[1, 2].scatter([v['train_time'] for v in nn_results.values()],\n",
        "                   [v['accuracy'] for v in nn_results.values()],\n",
        "                   s=200, alpha=0.6)\n",
        "for i, model in enumerate(nn_results.keys()):\n",
        "    axes[1, 2].annotate(model,\n",
        "                       ([v['train_time'] for v in nn_results.values()][i],\n",
        "                        [v['accuracy'] for v in nn_results.values()][i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1, 2].set_xlabel('Training Time (s)')\n",
        "axes[1, 2].set_ylabel('Accuracy')\n",
        "axes[1, 2].set_title('Accuracy vs Training Time Trade-off')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('neural_network_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save results for future reference\n",
        "results_dict = {\n",
        "    'comparison_table': nn_comparison_df.to_dict(),\n",
        "    'detailed_results': nn_results,\n",
        "    'metadata': {\n",
        "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'device': str(device),\n",
        "        'dataset_sizes': {\n",
        "            'train': len(X_temp),\n",
        "            'val': len(X_val),\n",
        "            'test': len(X_test_classical)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('neural_network_results.json', 'w') as f:\n",
        "    json.dump(results_dict, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\nResults saved to 'neural_network_results.json'\")\n",
        "print(\"Visualization saved to 'neural_network_comparison.png'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"THEORETICAL CONCLUSIONS - NEURAL NETWORK APPROACHES\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. MLP WITH TF-IDF FEATURES:\n",
        "   - Architecture: Classical neural network with TF-IDF representations\n",
        "   - Strengths:\n",
        "       * Simple and fast to train\n",
        "       * Can learn complex non-linear patterns from TF-IDF features\n",
        "       * Good baseline for comparison\n",
        "   - Limitations:\n",
        "       * Limited by TF-IDF's bag-of-words assumptions\n",
        "       * No understanding of word context or semantics\n",
        "       * High-dimensional sparse representations\n",
        "   - Best for: Quick prototyping, baseline comparisons\n",
        "\n",
        "2. BERT EMBEDDINGS + CLASSIFIER (Feature Extraction):\n",
        "   - Architecture: Frozen BERT + trainable classifier layers\n",
        "   - Strengths:\n",
        "       * Leverages pre-trained contextual embeddings\n",
        "       * Captures semantic meaning and word context\n",
        "       * Better generalization than TF-IDF\n",
        "       * Faster training than full fine-tuning\n",
        "   - Limitations:\n",
        "       * BERT weights are frozen, can't adapt to specific domain\n",
        "       * Still separated feature extraction and classification\n",
        "   - Best for: When you want contextual features but limited training resources\n",
        "\n",
        "3. BERT FINE-TUNING:\n",
        "   - Architecture: Complete BERT model with trainable classifier\n",
        "   - Strengths:\n",
        "       * Adapts pre-trained knowledge to specific task\n",
        "       * Captures nuanced semantic relationships\n",
        "       * State-of-the-art performance for NLP tasks\n",
        "       * End-to-end training\n",
        "   - Limitations:\n",
        "       * Computationally expensive\n",
        "       * Requires careful hyperparameter tuning\n",
        "       * Risk of overfitting on small datasets\n",
        "   - Best for: Achieving maximum performance, when sufficient data is available\n",
        "\n",
        "4. KEY INSIGHTS FROM OUR EXPERIMENT:\n",
        "   - Performance Trade-off: BERT fine-tuned > BERT embeddings > MLP (TF-IDF)\n",
        "   - Computational Cost: BERT fine-tuned > BERT embeddings > MLP (TF-IDF)\n",
        "   - Context Understanding: BERT models understand word context, TF-IDF does not\n",
        "   - Transfer Learning: BERT leverages knowledge from massive pre-training\n",
        "\n",
        "5. WHEN TO USE EACH APPROACH:\n",
        "   - Use MLP + TF-IDF: When speed is critical, dataset is small, or as a baseline\n",
        "   - Use BERT Embeddings: When you want better performance than TF-IDF but limited compute/time\n",
        "   - Use BERT Fine-tuning: When you need state-of-the-art performance and have sufficient resources\n",
        "\n",
        "6. PRACTICAL RECOMMENDATIONS:\n",
        "   - Start with simple models (MLP + TF-IDF) as baselines\n",
        "   - For moderate improvement: Try BERT embeddings\n",
        "   - For maximum performance: Fine-tune BERT\n",
        "   - Consider using DistilBERT or smaller variants if compute is limited\n",
        "   - Always monitor for overfitting with validation sets (haven't search yet why, but it seems really easy to overfit here)\n"
      ],
      "metadata": {
        "id": "Wyc2LoGE-A5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 : LLM-Based Classification"
      ],
      "metadata": {
        "id": "kyt3iKHa1mA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Zero-shot Classification with GPT/LLaMA"
      ],
      "metadata": {
        "id": "ImLQtPxh7R0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: LLM-BASED CLASSIFICATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 3: LLM-BASED CLASSIFICATION (Free Methods)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "In this section, we explore three LLM-based approaches:\n",
        "1. Zero-shot classification with free models (Hugging Face)\n",
        "2. Few-shot classification with prompt engineering\n",
        "3. Comparison with previous models\n",
        "\n",
        "Note: All methods are FREE and can run on Google Colab T4 GPU\n",
        "\"\"\")\n",
        "\n",
        "# Install required packages if not already installed\n",
        "print(\"\\nChecking/Installing required packages...\")\n",
        "!pip -q install accelerate bitsandbytes  # For efficient inference\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ZERO-SHOT CLASSIFICATION WITH HUGGING FACE MODELS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. ZERO-SHOT CLASSIFICATION (Free - No API Keys)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Approach: We'll use Hugging Face zero-shot classification models\n",
        "that are free and can run locally on Colab.\n",
        "\n",
        "Available options for Colab (with T4 GPU):\n",
        "1. facebook/bart-large-mnli - Good for zero-shot, ~1.6GB\n",
        "2. typeform/distilbert-base-uncased-mnli - Light and fast, ~250MB\n",
        "3. MoritzLaurer/deberta-v3-base-zeroshot-v1 - Specialized, ~500MB\n",
        "\"\"\")\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "    import torch\n",
        "\n",
        "    # Check available memory\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
        "        print(f\"\\nGPU Memory available: {gpu_memory:.1f} GB\")\n",
        "\n",
        "        if gpu_memory < 6:  # Colab T4 has ~15GB, but we need to be conservative\n",
        "            model_choice = \"typeform/distilbert-base-uncased-mnli\"\n",
        "            print(\"Selecting light model for limited memory...\")\n",
        "        else:\n",
        "            model_choice = \"facebook/bart-large-mnli\"\n",
        "            print(\"Selecting larger model for better accuracy...\")\n",
        "    else:\n",
        "        model_choice = \"typeform/distilbert-base-uncased-mnli\"\n",
        "        print(\"Using CPU - selecting lightest model...\")\n",
        "\n",
        "    print(f\"\\nLoading model: {model_choice}\")\n",
        "\n",
        "    # Use pipeline for easy zero-shot classification\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "    # Load the classifier\n",
        "    classifier = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=model_choice,\n",
        "        device=device,\n",
        "        framework=\"pt\"\n",
        "    )\n",
        "\n",
        "    print(\" Model loaded successfully!\")\n",
        "\n",
        "    # Candidate labels for sentiment\n",
        "    candidate_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    # Test on a sample from the dataset\n",
        "    print(\"\\nTesting zero-shot on a sample of 100 examples...\")\n",
        "\n",
        "    # Take a random sample\n",
        "    sample_size = min(100, len(test_df))\n",
        "    np.random.seed(42)\n",
        "    sample_indices = np.random.choice(len(test_df), sample_size, replace=False)\n",
        "\n",
        "    zero_shot_predictions = []\n",
        "    zero_shot_confidences = []\n",
        "    zero_shot_true_labels = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        text = test_df.iloc[idx]['text']\n",
        "\n",
        "        # Truncate if too long (models have token limits)\n",
        "        if len(text) > 500:\n",
        "            text = text[:500] + \"...\"\n",
        "\n",
        "        true_label = test_df.iloc[idx]['sentiment']\n",
        "\n",
        "        try:\n",
        "            # Perform zero-shot classification\n",
        "            result = classifier(text, candidate_labels, multi_label=False)\n",
        "\n",
        "            # Get top prediction\n",
        "            predicted_label = result['labels'][0]\n",
        "            confidence = result['scores'][0]\n",
        "\n",
        "            zero_shot_predictions.append(predicted_label)\n",
        "            zero_shot_confidences.append(confidence)\n",
        "            zero_shot_true_labels.append(true_label)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to neutral if classification fails\n",
        "            print(f\"Warning: Error on sample {i}: {str(e)[:100]}\")\n",
        "            zero_shot_predictions.append('neutral')\n",
        "            zero_shot_confidences.append(0.33)\n",
        "            zero_shot_true_labels.append(true_label)\n",
        "\n",
        "        # Show progress\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"  Processed {i + 1}/{sample_size} examples...\")\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Convert predictions to encoded format\n",
        "    predicted_encoded = label_encoder.transform(zero_shot_predictions)\n",
        "    true_encoded = label_encoder.transform(zero_shot_true_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    zero_shot_accuracy = accuracy_score(true_encoded, predicted_encoded)\n",
        "    zero_shot_precision, zero_shot_recall, zero_shot_f1, _ = precision_recall_fscore_support(\n",
        "        true_encoded, predicted_encoded, average='weighted'\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ZERO-SHOT CLASSIFICATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Model used: {model_choice}\")\n",
        "    print(f\"Sample size: {sample_size}\")\n",
        "    print(f\"Accuracy:  {zero_shot_accuracy:.4f}\")\n",
        "    print(f\"Precision: {zero_shot_precision:.4f}\")\n",
        "    print(f\"Recall:    {zero_shot_recall:.4f}\")\n",
        "    print(f\"F1-Score:  {zero_shot_f1:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.2f}s\")\n",
        "    print(f\"Time per sample: {inference_time/sample_size:.4f}s\")\n",
        "\n",
        "    # Calculate confidence statistics\n",
        "    avg_confidence = np.mean(zero_shot_confidences)\n",
        "    print(f\"Average confidence: {avg_confidence:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    llm_results = {\n",
        "        'Zero-shot (HF Model)': {\n",
        "            'accuracy': zero_shot_accuracy,\n",
        "            'precision': zero_shot_precision,\n",
        "            'recall': zero_shot_recall,\n",
        "            'f1': zero_shot_f1,\n",
        "            'train_time': 0,  # No training for zero-shot\n",
        "            'inference_time': inference_time,\n",
        "            'sample_size': sample_size,\n",
        "            'avg_confidence': avg_confidence,\n",
        "            'predictions': zero_shot_predictions,\n",
        "            'true_labels': zero_shot_true_labels,\n",
        "            'model_name': model_choice\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Show some examples\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EXAMPLE PREDICTIONS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for i in range(min(5, sample_size)):\n",
        "        idx = sample_indices[i]\n",
        "        text_preview = test_df.iloc[idx]['text'][:80] + \"...\" if len(test_df.iloc[idx]['text']) > 80 else test_df.iloc[idx]['text']\n",
        "        true = zero_shot_true_labels[i]\n",
        "        pred = zero_shot_predictions[i]\n",
        "        conf = zero_shot_confidences[i]\n",
        "\n",
        "        status = \"\" if true == pred else \"\"\n",
        "        print(f\"{status} Text: {text_preview}\")\n",
        "        print(f\"   True: {true} | Pred: {pred} | Conf: {conf:.3f}\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error in zero-shot classification: {e}\")\n",
        "    print(\"Trying alternative approach with smaller model...\")\n",
        "\n",
        "    try:\n",
        "        # Try with an even smaller model\n",
        "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "        import torch.nn.functional as F\n",
        "\n",
        "        print(\"\\nLoading cross-encoder model (smaller alternative)...\")\n",
        "\n",
        "        # Use a small cross-encoder model\n",
        "        model_name = \"cross-encoder/nli-distilroberta-base\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "\n",
        "        model.eval()\n",
        "        print(\" Alternative model loaded!\")\n",
        "\n",
        "        def zero_shot_classify_cross_encoder(text, labels):\n",
        "            \"\"\"Zero-shot classification using cross-encoder\"\"\"\n",
        "            results = []\n",
        "\n",
        "            for label in labels:\n",
        "                # Create hypothesis\n",
        "                hypothesis = f\"This text expresses {label} sentiment.\"\n",
        "\n",
        "                # Encode\n",
        "                inputs = tokenizer(text, hypothesis,\n",
        "                                 truncation=True, padding=True,\n",
        "                                 max_length=128,\n",
        "                                 return_tensors=\"pt\")\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "                # Predict\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                    probs = F.softmax(outputs.logits, dim=1)\n",
        "                    # For NLI models: entailment score is usually index 0\n",
        "                    score = probs[0][0].item()\n",
        "\n",
        "                results.append((label, score))\n",
        "\n",
        "            # Return label with highest entailment score\n",
        "            results.sort(key=lambda x: x[1], reverse=True)\n",
        "            return results[0][0], results[0][1]\n",
        "\n",
        "        # Test on smaller sample\n",
        "        sample_size = min(50, len(test_df))\n",
        "        np.random.seed(42)\n",
        "        sample_indices = np.random.choice(len(test_df), sample_size, replace=False)\n",
        "\n",
        "        zero_shot_predictions = []\n",
        "        zero_shot_confidences = []\n",
        "        zero_shot_true_labels = []\n",
        "        candidate_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            text = test_df.iloc[idx]['text'][:300]  # Shorter text\n",
        "            true_label = test_df.iloc[idx]['sentiment']\n",
        "\n",
        "            try:\n",
        "                predicted_label, confidence = zero_shot_classify_cross_encoder(text, candidate_labels)\n",
        "\n",
        "                zero_shot_predictions.append(predicted_label)\n",
        "                zero_shot_confidences.append(confidence)\n",
        "                zero_shot_true_labels.append(true_label)\n",
        "\n",
        "            except Exception as e:\n",
        "                zero_shot_predictions.append('neutral')\n",
        "                zero_shot_confidences.append(0.33)\n",
        "                zero_shot_true_labels.append(true_label)\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  Processed {i + 1}/{sample_size} examples...\")\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        predicted_encoded = label_encoder.transform(zero_shot_predictions)\n",
        "        true_encoded = label_encoder.transform(zero_shot_true_labels)\n",
        "\n",
        "        zero_shot_accuracy = accuracy_score(true_encoded, predicted_encoded)\n",
        "        zero_shot_precision, zero_shot_recall, zero_shot_f1, _ = precision_recall_fscore_support(\n",
        "            true_encoded, predicted_encoded, average='weighted'\n",
        "        )\n",
        "\n",
        "        print(f\"\\nZero-shot Classification Results (Cross-Encoder):\")\n",
        "        print(f\"Accuracy:  {zero_shot_accuracy:.4f}\")\n",
        "        print(f\"Precision: {zero_shot_precision:.4f}\")\n",
        "        print(f\"Recall:    {zero_shot_recall:.4f}\")\n",
        "        print(f\"F1-Score:  {zero_shot_f1:.4f}\")\n",
        "        print(f\"Inference Time: {inference_time:.2f}s\")\n",
        "\n",
        "        llm_results = {\n",
        "            'Zero-shot (Cross-Encoder)': {\n",
        "                'accuracy': zero_shot_accuracy,\n",
        "                'precision': zero_shot_precision,\n",
        "                'recall': zero_shot_recall,\n",
        "                'f1': zero_shot_f1,\n",
        "                'train_time': 0,\n",
        "                'inference_time': inference_time,\n",
        "                'sample_size': sample_size,\n",
        "                'avg_confidence': np.mean(zero_shot_confidences),\n",
        "                'predictions': zero_shot_predictions,\n",
        "                'true_labels': zero_shot_true_labels,\n",
        "                'model_name': model_name\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"\\n Could not load any zero-shot models: {e2}\")\n",
        "        print(\"Using keyword-based baseline instead...\")\n",
        "\n",
        "        # Simple keyword baseline as fallback\n",
        "        def keyword_baseline_classifier(text):\n",
        "            text_lower = text.lower()\n",
        "\n",
        "            positive_words = ['good', 'great', 'excellent', 'amazing', 'love',\n",
        "                            'best', 'fantastic', 'wonderful', 'happy', 'perfect',\n",
        "                            'awesome', 'brilliant', 'outstanding', 'superb']\n",
        "            negative_words = ['bad', 'terrible', 'awful', 'worst', 'hate',\n",
        "                            'disappointing', 'poor', 'horrible', 'boring', 'sad',\n",
        "                            'waste', 'rubbish', 'trash', 'disgusting']\n",
        "\n",
        "            pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "            neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "            if pos_count > neg_count:\n",
        "                return 'positive', pos_count/(pos_count+neg_count+0.1)\n",
        "            elif neg_count > pos_count:\n",
        "                return 'negative', neg_count/(pos_count+neg_count+0.1)\n",
        "            else:\n",
        "                return 'neutral', 0.5\n",
        "\n",
        "        # Test baseline\n",
        "        sample_size = min(200, len(test_df))\n",
        "        sample_indices = np.random.choice(len(test_df), sample_size, replace=False)\n",
        "\n",
        "        baseline_predictions = []\n",
        "        baseline_confidences = []\n",
        "        baseline_true_labels = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx in sample_indices:\n",
        "            text = test_df.iloc[idx]['text']\n",
        "            true_label = test_df.iloc[idx]['sentiment']\n",
        "\n",
        "            predicted, confidence = keyword_baseline_classifier(text)\n",
        "\n",
        "            baseline_predictions.append(predicted)\n",
        "            baseline_confidences.append(confidence)\n",
        "            baseline_true_labels.append(true_label)\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        predicted_encoded = label_encoder.transform(baseline_predictions)\n",
        "        true_encoded = label_encoder.transform(baseline_true_labels)\n",
        "\n",
        "        baseline_accuracy = accuracy_score(true_encoded, predicted_encoded)\n",
        "        baseline_precision, baseline_recall, baseline_f1, _ = precision_recall_fscore_support(\n",
        "            true_encoded, predicted_encoded, average='weighted'\n",
        "        )\n",
        "\n",
        "        print(f\"\\nKeyword Baseline Results:\")\n",
        "        print(f\"Accuracy:  {baseline_accuracy:.4f}\")\n",
        "        print(f\"Precision: {baseline_precision:.4f}\")\n",
        "        print(f\"Recall:    {baseline_recall:.4f}\")\n",
        "        print(f\"F1-Score:  {baseline_f1:.4f}\")\n",
        "        print(f\"Inference Time: {inference_time:.4f}s\")\n",
        "\n",
        "        llm_results = {\n",
        "            'Keyword Baseline': {\n",
        "                'accuracy': baseline_accuracy,\n",
        "                'precision': baseline_precision,\n",
        "                'recall': baseline_recall,\n",
        "                'f1': baseline_f1,\n",
        "                'train_time': 0,\n",
        "                'inference_time': inference_time,\n",
        "                'sample_size': sample_size,\n",
        "                'avg_confidence': np.mean(baseline_confidences),\n",
        "                'predictions': baseline_predictions,\n",
        "                'true_labels': baseline_true_labels,\n",
        "                'model_name': 'Keyword-based'\n",
        "            }\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "p5LQ2B7hDG2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Few-shot/Prompt Engineering"
      ],
      "metadata": {
        "id": "3B36jssx7Uz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. FEW-SHOT LEARNING WITH LOCAL LLMS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. FEW-SHOT LEARNING (With Local LLM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Approach: We'll use a small, efficient LLM for few-shot learning.\n",
        "Option 1: DistilGPT-2 (lightweight, 82M parameters)\n",
        "Option 2: Google's T5-small (good for text-to-text tasks)\n",
        "\"\"\")\n",
        "\n",
        "try:\n",
        "    # Let's use T5-small which is better for classification tasks\n",
        "    print(\"\\nLoading T5-small for few-shot learning...\")\n",
        "\n",
        "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "    # Load T5-small\n",
        "    model_name = \"t5-small\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    model.eval()\n",
        "    print(\" T5-small loaded successfully!\")\n",
        "\n",
        "    def create_few_shot_prompt_t5(text, examples=3):\n",
        "        \"\"\"Create few-shot prompt for T5\"\"\"\n",
        "        # Example few-shot demonstrations\n",
        "        few_shot_examples = [\n",
        "            (\"I absolutely love this product! It's amazing.\", \"positive\"),\n",
        "            (\"This is the worst service I've ever experienced.\", \"negative\"),\n",
        "            (\"The item arrived on time and works as expected.\", \"neutral\"),\n",
        "            (\"Not bad, but could be better.\", \"neutral\"),\n",
        "            (\"Absolutely terrible, would not recommend.\", \"negative\"),\n",
        "            (\"Best purchase I've made this year!\", \"positive\")\n",
        "        ]\n",
        "\n",
        "        # Select random examples\n",
        "        np.random.seed(42)\n",
        "        selected_examples = few_shot_examples[:examples]\n",
        "\n",
        "        # Build prompt\n",
        "        prompt = \"Classify the sentiment of the text as positive, negative, or neutral.\\n\\n\"\n",
        "\n",
        "        for example_text, example_label in selected_examples:\n",
        "            prompt += f\"Text: {example_text}\\n\"\n",
        "            prompt += f\"Sentiment: {example_label}\\n\\n\"\n",
        "\n",
        "        prompt += f\"Text: {text}\\n\"\n",
        "        prompt += \"Sentiment:\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def classify_with_t5(text, model, tokenizer):\n",
        "        \"\"\"Classify using T5\"\"\"\n",
        "        prompt = create_few_shot_prompt_t5(text)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                num_beams=3,\n",
        "                temperature=0.7,\n",
        "                do_sample=False,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract just the sentiment part\n",
        "        response = generated.lower()\n",
        "\n",
        "        # Parse response\n",
        "        if 'positive' in response:\n",
        "            return 'positive'\n",
        "        elif 'negative' in response:\n",
        "            return 'negative'\n",
        "        elif 'neutral' in response:\n",
        "            return 'neutral'\n",
        "        else:\n",
        "            # Default to neutral if unclear\n",
        "            return 'neutral'\n",
        "\n",
        "    # Test on a small sample (T5 can be slow)\n",
        "    sample_size = min(30, len(test_df))\n",
        "    sample_indices = np.random.choice(len(test_df), sample_size, replace=False)\n",
        "\n",
        "    few_shot_predictions = []\n",
        "    few_shot_true_labels = []\n",
        "\n",
        "    print(f\"\\nTesting few-shot learning on {sample_size} examples...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        text = test_df.iloc[idx]['text'][:200]  # Limit text length\n",
        "        true_label = test_df.iloc[idx]['sentiment']\n",
        "\n",
        "        try:\n",
        "            predicted = classify_with_t5(text, model, tokenizer)\n",
        "            few_shot_predictions.append(predicted)\n",
        "            few_shot_true_labels.append(true_label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on example {i}: {e}\")\n",
        "            few_shot_predictions.append('neutral')\n",
        "            few_shot_true_labels.append(true_label)\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f\"  Processed {i + 1}/{sample_size} examples...\")\n",
        "\n",
        "    few_shot_time = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    predicted_encoded = label_encoder.transform(few_shot_predictions)\n",
        "    true_encoded = label_encoder.transform(few_shot_true_labels)\n",
        "\n",
        "    few_shot_accuracy = accuracy_score(true_encoded, predicted_encoded)\n",
        "    few_shot_precision, few_shot_recall, few_shot_f1, _ = precision_recall_fscore_support(\n",
        "        true_encoded, predicted_encoded, average='weighted'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFew-shot Learning Results (T5-small):\")\n",
        "    print(f\"Accuracy:  {few_shot_accuracy:.4f}\")\n",
        "    print(f\"Precision: {few_shot_precision:.4f}\")\n",
        "    print(f\"Recall:    {few_shot_recall:.4f}\")\n",
        "    print(f\"F1-Score:  {few_shot_f1:.4f}\")\n",
        "    print(f\"Inference Time: {few_shot_time:.2f}s\")\n",
        "\n",
        "    # Add to results\n",
        "    llm_results['Few-shot (T5-small)'] = {\n",
        "        'accuracy': few_shot_accuracy,\n",
        "        'precision': few_shot_precision,\n",
        "        'recall': few_shot_recall,\n",
        "        'f1': few_shot_f1,\n",
        "        'train_time': 0,\n",
        "        'inference_time': few_shot_time,\n",
        "        'sample_size': sample_size,\n",
        "        'predictions': few_shot_predictions,\n",
        "        'true_labels': few_shot_true_labels,\n",
        "        'model_name': 't5-small'\n",
        "    }\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Could not run few-shot learning: {e}\")\n",
        "    print(\"Skipping few-shot approach...\")"
      ],
      "metadata": {
        "id": "W6ZsZanIDHci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Comparison with previous models"
      ],
      "metadata": {
        "id": "hjMNo8Bx7WY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. COMPARISON WITH PREVIOUS MODELS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. COMPARISON WITH PREVIOUS MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First, define the categorize_model function BEFORE using it\n",
        "def categorize_model(model_name):\n",
        "    \"\"\"Categorize model into Classical ML, Neural Network, or LLM-based\"\"\"\n",
        "    model_name_lower = model_name.lower()\n",
        "    if 'zero-shot' in model_name_lower or 'few-shot' in model_name_lower or 'keyword' in model_name_lower:\n",
        "        return 'LLM-based'\n",
        "    elif 'bert' in model_name_lower or 'mlp' in model_name_lower:\n",
        "        return 'Neural Network'\n",
        "    else:\n",
        "        return 'Classical ML'\n",
        "\n",
        "# Combine all results from previous sections\n",
        "if 'llm_results' in locals():\n",
        "    all_results = {**classical_results, **nn_results, **llm_results}\n",
        "else:\n",
        "    all_results = {**classical_results, **nn_results}\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "for model_name, results in all_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': results['accuracy'],\n",
        "        'F1-Score': results['f1'],\n",
        "        'Training Time (s)': results.get('train_time', 0),\n",
        "        'Inference Time (s)': results.get('inference_time', 0),\n",
        "        'Approach': categorize_model(model_name)  # Now this function is defined\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# We don't need to categorize again since we already added 'Approach' column\n",
        "# Just rename it to 'Category' for consistency with the visualization code\n",
        "comparison_df['Category'] = comparison_df['Approach']\n",
        "\n",
        "# Sort by F1-Score\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\nCOMPREHENSIVE MODEL COMPARISON:\")\n",
        "print(\"-\" * 80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# The rest of your visualization code remains the same...\n",
        "# Visualization\n",
        "print(\"\\nGenerating comparison visualization...\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# F1-Score comparison (color by category)\n",
        "categories = comparison_df['Category'].unique()\n",
        "category_colors = {'Classical ML': 'skyblue', 'Neural Network': 'lightgreen', 'LLM-based': 'salmon'}\n",
        "\n",
        "for category in categories:\n",
        "    category_data = comparison_df[comparison_df['Category'] == category]\n",
        "    axes[0, 0].barh(category_data['Model'], category_data['F1-Score'],\n",
        "                    color=category_colors[category], label=category)\n",
        "\n",
        "axes[0, 0].set_xlabel('F1-Score')\n",
        "axes[0, 0].set_title('Model Comparison by F1-Score (Colored by Approach)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_xlim([0, 1])\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "\n",
        "# Accuracy vs Inference Time\n",
        "for category in categories:\n",
        "    category_data = comparison_df[comparison_df['Category'] == category]\n",
        "    axes[0, 1].scatter(category_data['Inference Time (s)'], category_data['Accuracy'],\n",
        "                      s=100, alpha=0.7, label=category, c=category_colors[category])\n",
        "\n",
        "axes[0, 1].set_xlabel('Inference Time (seconds) - Log scale')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Accuracy vs Inference Time Trade-off')\n",
        "axes[0, 1].set_xscale('log')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Performance by approach (box plot)\n",
        "performance_data = []\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    performance_data.append({\n",
        "        'Approach': row['Category'],\n",
        "        'F1-Score': row['F1-Score'],\n",
        "        'Accuracy': row['Accuracy']\n",
        "    })\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "# Box plot for F1-Score by approach\n",
        "axes[1, 0].boxplot([performance_df[performance_df['Approach'] == cat]['F1-Score'].values\n",
        "                    for cat in categories],\n",
        "                   labels=categories)\n",
        "axes[1, 0].set_ylabel('F1-Score')\n",
        "axes[1, 0].set_title('Performance Distribution by Approach (F1-Score)')\n",
        "axes[1, 0].set_ylim([0, 1])\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Radar chart for top 3 models\n",
        "top_3_models = comparison_df.head(3)['Model'].tolist()\n",
        "metrics = ['Accuracy', 'F1-Score', 'Inference Speed', 'Training Speed']\n",
        "\n",
        "# Normalize metrics for radar chart\n",
        "def normalize_metric(value, max_val, min_val=0, invert=False):\n",
        "    \"\"\"Normalize metric to 0-1 range\"\"\"\n",
        "    if invert:  # For time metrics where lower is better\n",
        "        value = max_val - value + min_val\n",
        "    normalized = (value - min_val) / (max_val - min_val)\n",
        "    return max(0, min(1, normalized))\n",
        "\n",
        "axes[1, 1].set_title('Top 3 Models - Radar Chart Comparison', pad=20)\n",
        "\n",
        "# Prepare data for radar chart\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Close the polygon\n",
        "\n",
        "for i, model_name in enumerate(top_3_models):\n",
        "    model_data = comparison_df[comparison_df['Model'] == model_name].iloc[0]\n",
        "\n",
        "    # Normalize each metric\n",
        "    values = [\n",
        "        model_data['Accuracy'],  # Accuracy\n",
        "        model_data['F1-Score'],  # F1-Score\n",
        "        normalize_metric(model_data['Inference Time (s)'],\n",
        "                        comparison_df['Inference Time (s)'].max(),\n",
        "                        invert=True),  # Inference Speed (higher is better)\n",
        "        normalize_metric(model_data['Training Time (s)'],\n",
        "                        comparison_df['Training Time (s)'].max(),\n",
        "                        invert=True)   # Training Speed (higher is better)\n",
        "    ]\n",
        "\n",
        "    values += values[:1]  # Close the polygon\n",
        "\n",
        "    # Plot\n",
        "    axes[1, 1].plot(angles, values, 'o-', linewidth=2, label=model_name)\n",
        "    axes[1, 1].fill(angles, values, alpha=0.1)\n",
        "\n",
        "axes[1, 1].set_xticks(angles[:-1])\n",
        "axes[1, 1].set_xticklabels(metrics)\n",
        "axes[1, 1].set_ylim([0, 1])\n",
        "axes[1, 1].grid(True)\n",
        "axes[1, 1].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llm_comparison_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS - LLM vs TRADITIONAL APPROACHES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. ZERO-SHOT LLM PERFORMANCE:\n",
        "   - Achieved {llm_results.get('Zero-shot (HF Model)', llm_results.get('Zero-shot (Cross-Encoder)', llm_results.get('Keyword Baseline', {}))).get('f1', 0):.4f} F1-Score\n",
        "   - No training required - uses pre-trained knowledge\n",
        "   - Good for quick prototyping and small datasets\n",
        "   - Can understand nuanced language without task-specific training\n",
        "\n",
        "2. FEW-SHOT LEARNING:\n",
        "   - Shows potential with limited examples\n",
        "   - More flexible than zero-shot\n",
        "   - Slower inference compared to classical methods\n",
        "\n",
        "3. COMPARATIVE ANALYSIS:\n",
        "   - Classical ML: Fastest training and inference, good baseline\n",
        "   - Neural Networks: Better accuracy, understands context\n",
        "   - LLM-based: No training needed, leverages pre-trained knowledge\n",
        "\n",
        "4. PRACTICAL RECOMMENDATIONS:\n",
        "   - For production: BERT fine-tuned or Logistic Regression\n",
        "   - For rapid prototyping: Zero-shot LLM or Naive Bayes\n",
        "   - For limited data: BERT embeddings or zero-shot LLM\n",
        "   - For real-time: Logistic Regression or Linear SVM\n",
        "\n",
        "5. COST-BENEFIT TRADEOFF:\n",
        "   - Zero-shot LLM: Free, no training, slower inference\n",
        "   - Classical ML: Fast, efficient, requires training data\n",
        "   - BERT-based: Best accuracy, higher computational cost\n",
        "\"\"\")\n",
        "\n",
        "# Save comprehensive results\n",
        "comprehensive_results = {\n",
        "    'all_models': all_results,\n",
        "    'comparison_table': comparison_df.to_dict(),\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'summary': {\n",
        "        'best_model': comparison_df.iloc[0]['Model'],\n",
        "        'best_f1': comparison_df.iloc[0]['F1-Score'],\n",
        "        'best_accuracy': comparison_df.iloc[0]['Accuracy'],\n",
        "        'fastest_inference': comparison_df.loc[comparison_df['Inference Time (s)'].idxmin()]['Model'],\n",
        "        'best_llm': comparison_df[comparison_df['Category'] == 'LLM-based'].iloc[0]['Model'] if 'LLM-based' in comparison_df['Category'].values else 'None'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('comprehensive_results.json', 'w') as f:\n",
        "    json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "BEST OVERALL MODEL: {comparison_df.iloc[0]['Model']}\n",
        "- F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\n",
        "- Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\n",
        "- Approach: {comparison_df.iloc[0]['Category']}\n",
        "\n",
        "BEST LLM APPROACH: {comprehensive_results['summary']['best_llm']}\n",
        "\n",
        "RECOMMENDED STRATEGY BY USE CASE:\n",
        "1. MAXIMUM ACCURACY: {comparison_df.iloc[0]['Model']}\n",
        "2. REAL-TIME PROCESSING: {comparison_df.loc[comparison_df['Inference Time (s)'].idxmin()]['Model']}\n",
        "3. LIMITED TRAINING DATA: Zero-shot LLM or BERT embeddings\n",
        "4. RESOURCE-CONSTRAINED: Logistic Regression or Naive Bayes\n",
        "5. NO TRAINING ALLOWED: Zero-shot classification with Hugging Face models\n",
        "\n",
        "Results saved to: 'comprehensive_results.json'\n",
        "Visualization saved to: 'llm_comparison_results.png'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "u4lhTfYDDIHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSqlO1oIyXK3"
      },
      "source": [
        "# Section 4 : Comprehensive Analysis & Conclusions\n",
        "\n",
        "1. All Models Perfomance Comparison Table:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# SECTION 4: COMPREHENSIVE ANALYSIS & CONCLUSIONS\n",
        "# All models comparison, error analysis, and BERT architecture explanation\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ALL MODELS PERFORMANCE COMPARISON TABLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON - ALL APPROACHES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Combine all results\n",
        "all_results = {**classical_results, **nn_results}\n",
        "\n",
        "comprehensive_df = pd.DataFrame({\n",
        "    'Model': list(all_results.keys()),\n",
        "    'Accuracy': [v['accuracy'] for v in all_results.values()],\n",
        "    'Precision': [v['precision'] for v in all_results.values()],\n",
        "    'Recall': [v['recall'] for v in all_results.values()],\n",
        "    'F1-Score': [v['f1'] for v in all_results.values()],\n",
        "    'Train Time (s)': [v['train_time'] for v in all_results.values()],\n",
        "    'Inference Time (s)': [v['inference_time'] for v in all_results.values()]\n",
        "})\n",
        "\n",
        "comprehensive_df = comprehensive_df.sort_values('F1-Score', ascending=False)\n",
        "print(comprehensive_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# F1-Score comparison\n",
        "sorted_df = comprehensive_df.sort_values('F1-Score')\n",
        "colors = ['red' if 'Naive' in m or 'Random' in m else\n",
        "          'orange' if 'SVM' in m or 'Logistic' in m or 'XGBoost' in m else\n",
        "          'blue' if 'MLP' in m else 'green'\n",
        "          for m in sorted_df['Model']]\n",
        "\n",
        "axes[0, 0].barh(sorted_df['Model'], sorted_df['F1-Score'], color=colors)\n",
        "axes[0, 0].set_xlabel('F1-Score')\n",
        "axes[0, 0].set_title('Model F1-Score Comparison (All Models)')\n",
        "axes[0, 0].set_xlim([0, 1])\n",
        "\n",
        "# Accuracy vs Training Time\n",
        "axes[0, 1].scatter(comprehensive_df['Train Time (s)'], comprehensive_df['Accuracy'], s=100)\n",
        "for idx, row in comprehensive_df.iterrows():\n",
        "    axes[0, 1].annotate(row['Model'], (row['Train Time (s)'], row['Accuracy']),\n",
        "                        fontsize=8, ha='right')\n",
        "axes[0, 1].set_xlabel('Training Time (s)')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Accuracy vs Training Time Trade-off')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Performance metrics comparison (top 3 models)\n",
        "top_3 = comprehensive_df.head(3)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, (_, row) in enumerate(top_3.iterrows()):\n",
        "    values = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]\n",
        "    axes[1, 0].bar(x + i * width, values, width, label=row['Model'])\n",
        "\n",
        "axes[1, 0].set_xlabel('Metrics')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Top 3 Models - Detailed Metrics Comparison')\n",
        "axes[1, 0].set_xticks(x + width)\n",
        "axes[1, 0].set_xticklabels(metrics)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].set_ylim([0, 1])\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Inference time comparison\n",
        "sorted_inf = comprehensive_df.sort_values('Inference Time (s)')\n",
        "axes[1, 1].barh(sorted_inf['Model'], sorted_inf['Inference Time (s)'], color='purple')\n",
        "axes[1, 1].set_xlabel('Inference Time (s)')\n",
        "axes[1, 1].set_title('Model Inference Time Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "boU5kHJx1Xx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzSO0nEgOuIN"
      },
      "source": [
        "2. Error Analysis (confusions matrices, misclassified samples):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txhRuSbWO3Kk"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. ERROR ANALYSIS - CONFUSION MATRICES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ERROR ANALYSIS - CONFUSION MATRICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Plot confusion matrices for top 3 models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "top_3_models = comprehensive_df.head(3)['Model'].tolist()\n",
        "\n",
        "for idx, model_name in enumerate(top_3_models):\n",
        "    if 'confusion_matrix' in all_results[model_name]:\n",
        "        cm = all_results[model_name]['confusion_matrix']\n",
        "    else:\n",
        "        # Calculate confusion matrix for neural network models\n",
        "        cm = confusion_matrix(y_test_classical, all_results[model_name]['predictions'])\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=sentiment_labels, yticklabels=sentiment_labels,\n",
        "                ax=axes[idx])\n",
        "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze misclassifications\n",
        "print(\"\\nMisclassification Analysis for Best Model:\")\n",
        "best_model_name = comprehensive_df.iloc[0]['Model']\n",
        "best_predictions = all_results[best_model_name]['predictions']\n",
        "\n",
        "misclassified_indices = np.where(best_predictions != y_test_classical.values)[0]\n",
        "print(f\"\\nTotal misclassified: {len(misclassified_indices)} out of {len(y_test_classical)}\")\n",
        "print(f\"Error rate: {len(misclassified_indices)/len(y_test_classical)*100:.2f}%\")\n",
        "\n",
        "# Show some examples of misclassifications\n",
        "print(\"\\nExample Misclassifications:\")\n",
        "for i in misclassified_indices[:5]:\n",
        "    true_label = label_encoder.inverse_transform([y_test_classical.values[i]])[0]\n",
        "    pred_label = label_encoder.inverse_transform([best_predictions[i]])[0]\n",
        "    text = test_df.iloc[i]['text'][:100]  # First 100 chars\n",
        "    print(f\"\\nText: {text}...\")\n",
        "    print(f\"True: {true_label} | Predicted: {pred_label}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMHPazWO1Bv"
      },
      "source": [
        "3. Computational Cost Analysis (training/inference time):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsS2JexWO614"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. COMPUTATIONAL COST ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPUTATIONAL COST ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cost_df = comprehensive_df[['Model', 'Train Time (s)', 'Inference Time (s)']].copy()\n",
        "cost_df['Inference per Sample (ms)'] = (cost_df['Inference Time (s)'] / len(y_test_classical)) * 1000\n",
        "cost_df['Total Cost (s)'] = cost_df['Train Time (s)'] + cost_df['Inference Time (s)']\n",
        "\n",
        "print(cost_df.to_string(index=False))\n",
        "\n",
        "print(\"\"\"\n",
        "\\nCOST ANALYSIS INSIGHTS:\n",
        "\n",
        "1. FASTEST TRAINING:\n",
        "   - Naive Bayes: Extremely fast, good for rapid prototyping\n",
        "   - Logistic Regression: Very fast, production-ready baseline\n",
        "\n",
        "2. FASTEST INFERENCE:\n",
        "   - Classical ML models: Sub-second inference on test set\n",
        "   - Important for real-time applications\n",
        "\n",
        "3. MOST EXPENSIVE:\n",
        "   - BERT-based models: Higher computational cost but better performance\n",
        "   - Trade-off: accuracy vs. speed\n",
        "\"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpj_QJyAO7D_"
      },
      "source": [
        "4. Why BERT embedding?:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvxjrXqaPAKz"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. WHY BERT EMBEDDINGS INSTEAD OF RAW TEXT?\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"WHY BERT EMBEDDINGS INSTEAD OF RAW TEXT?\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "BERT EMBEDDINGS VS. RAW TEXT REPRESENTATIONS\n",
        "============================================\n",
        "\n",
        "1. CONTEXTUAL vs. STATIC REPRESENTATIONS:\n",
        "\n",
        "   RAW TEXT (Bag-of-Words, TF-IDF):\n",
        "   - Each word has ONE fixed representation regardless of context\n",
        "   - \"bank\" in \"river bank\" = \"bank\" in \"money bank\"\n",
        "   - No understanding of word order or relationships\n",
        "   - Sparse, high-dimensional vectors\n",
        "\n",
        "   BERT EMBEDDINGS:\n",
        "   - Same word gets DIFFERENT representations based on context\n",
        "   - \"bank\" in \"river bank\"  \"bank\" in \"money bank\"\n",
        "   - Captures bidirectional context (looks left AND right)\n",
        "   - Dense, semantic vectors (768 dimensions)\n",
        "\n",
        "2. SEMANTIC UNDERSTANDING:\n",
        "\n",
        "   RAW TEXT:\n",
        "   - Only counts word frequency\n",
        "   - \"not good\" vs \"good\" - can't understand negation\n",
        "   - Misses sarcasm, idioms, metaphors\n",
        "\n",
        "   BERT EMBEDDINGS:\n",
        "   - Understands semantic relationships\n",
        "   - Handles negation: \"not good\"  \"good\"\n",
        "   - Better at sarcasm detection\n",
        "   - Captures subtle sentiment nuances\n",
        "\n",
        "3. PRE-TRAINING ADVANTAGE:\n",
        "\n",
        "   RAW TEXT:\n",
        "   - Learned only from your dataset\n",
        "   - Limited by dataset size and diversity\n",
        "\n",
        "   BERT EMBEDDINGS:\n",
        "   - Pre-trained on 3.3 billion words (Wikipedia + Books)\n",
        "   - Transfer learning: brings external knowledge\n",
        "   - Works well even with small datasets\n",
        "\n",
        "4. WORD RELATIONSHIPS:\n",
        "\n",
        "   RAW TEXT:\n",
        "   - \"happy\" and \"joyful\" are completely unrelated\n",
        "   - No similarity between synonyms\n",
        "\n",
        "   BERT EMBEDDINGS:\n",
        "   - \"happy\" and \"joyful\" have similar embeddings\n",
        "   - Understands synonyms, antonyms, related concepts\n",
        "   - Semantic similarity is preserved\n",
        "\n",
        "5. HANDLING OUT-OF-VOCABULARY:\n",
        "\n",
        "   RAW TEXT:\n",
        "   - Unknown words = ignored or error\n",
        "   - New words not in training vocabulary\n",
        "\n",
        "   BERT EMBEDDINGS:\n",
        "   - Uses WordPiece tokenization\n",
        "   - Breaks unknown words into subwords\n",
        "   - \"unbelievable\" = \"un\" + \"##believable\"\n",
        "   - Can handle misspellings better\n",
        "\n",
        "PRACTICAL EXAMPLE:\n",
        "==================\n",
        "Sentence: \"This movie wasn't bad at all!\"\n",
        "\n",
        "RAW TEXT REPRESENTATION (TF-IDF):\n",
        "- Sees: [this, movie, wasn't, bad, all]\n",
        "- \"bad\" has negative weight  predicts NEGATIVE (WRONG!)\n",
        "- Misses the negation\n",
        "\n",
        "BERT REPRESENTATION:\n",
        "- Understands \"wasn't bad\" = positive sentiment\n",
        "- Context: \"at all\" reinforces the negation\n",
        "- Correctly predicts: POSITIVE \n",
        "\n",
        "DISADVANTAGES OF BERT EMBEDDINGS:\n",
        "==================================\n",
        "1. Computational cost: Slower than TF-IDF\n",
        "2. Memory intensive: Requires GPU for large datasets\n",
        "3. More complex: Harder to interpret\n",
        "4. Overkill for simple tasks\n",
        "\n",
        "WHEN TO USE WHAT:\n",
        "==================\n",
        "- Simple sentiment with clear positive/negative words  TF-IDF\n",
        "- Nuanced sentiment with sarcasm/negation  BERT\n",
        "- Real-time applications  TF-IDF + fast classifier\n",
        "- Highest accuracy requirement  BERT embeddings\n",
        "- Limited computational resources  TF-IDF\n",
        "- Small dataset  BERT (transfer learning helps)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Empirical demonstration\n",
        "print(\"EMPIRICAL DEMONSTRATION ON OUR DATASET:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"TF-IDF + Best Classical ML: {classical_results['Linear SVM']['f1']:.4f} F1-Score\")\n",
        "print(f\"BERT Embeddings + Classifier: {nn_results['BERT Embeddings + Classifier']['f1']:.4f} F1-Score\")\n",
        "print(f\"Improvement: {(nn_results['BERT Embeddings + Classifier']['f1'] - classical_results['Linear SVM']['f1'])*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTrzzpwIPAcS"
      },
      "source": [
        "5. BERT Architecture Explaination:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. BERT ARCHITECTURE EXPLANATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BERT ARCHITECTURE EXPLANATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "BERT: Bidirectional Encoder Representations from Transformers\n",
        "==============================================================\n",
        "(Devlin et al., 2018 - Google AI Language)\n",
        "\n",
        "OVERVIEW:\n",
        "=========\n",
        "BERT is a transformer-based model designed to pre-train deep bidirectional\n",
        "representations by jointly conditioning on both left and right context in\n",
        "all layers.\n",
        "\n",
        "KEY INNOVATION:\n",
        "===============\n",
        "Previous models (like GPT) were unidirectional - they could only look at\n",
        "previous words. BERT looks at BOTH directions simultaneously using a\n",
        "clever training approach.\n",
        "\n",
        "ARCHITECTURE COMPONENTS:\n",
        "========================\n",
        "\n",
        "1. TRANSFORMER ENCODER:\n",
        "   - Built entirely from transformer encoder blocks\n",
        "   - No decoder (unlike original Transformer)\n",
        "   - BERT-base: 12 encoder layers, 768 hidden dimensions\n",
        "   - BERT-large: 24 encoder layers, 1024 hidden dimensions\n",
        "\n",
        "2. SELF-ATTENTION MECHANISM:\n",
        "\n",
        "   For each word, attention allows the model to look at other words:\n",
        "\n",
        "   Attention(Q, K, V) = softmax(QK^T / d_k)  V\n",
        "\n",
        "   Where:\n",
        "   - Q (Query): \"What am I looking for?\"\n",
        "   - K (Key): \"What do I contain?\"\n",
        "   - V (Value): \"What information do I have?\"\n",
        "\n",
        "   Example: \"The bank by the river\"\n",
        "   - \"bank\" attends to \"river\" (high attention weight)\n",
        "   - Learns \"bank\" = river bank, not money bank\n",
        "\n",
        "3. MULTI-HEAD ATTENTION:\n",
        "\n",
        "   - 12 attention heads in BERT-base\n",
        "   - Each head learns different relationships\n",
        "   - Head 1: grammatical relationships\n",
        "   - Head 2: semantic relationships\n",
        "   - Head 3: long-distance dependencies\n",
        "   - etc.\n",
        "\n",
        "   Final output = Concat(head, head, ..., head)  W^O\n",
        "\n",
        "4. POSITION ENCODINGS:\n",
        "\n",
        "   - Transformers have no inherent word order\n",
        "   - BERT adds learned positional embeddings\n",
        "   - Position 0, 1, 2, ... each has unique vector\n",
        "   - Allows model to use word position information\n",
        "\n",
        "5. LAYER STRUCTURE:\n",
        "\n",
        "   Each of 12 layers contains:\n",
        "\n",
        "   Input  Multi-Head Attention  Add & Norm \n",
        "   Feed-Forward Network  Add & Norm  Output\n",
        "\n",
        "   - Add & Norm: Residual connection + Layer Normalization\n",
        "   - Feed-Forward: Two linear layers with GELU activation\n",
        "   - Hidden size: 768  3072  768\n",
        "\n",
        "PRE-TRAINING TASKS:\n",
        "===================\n",
        "\n",
        "BERT is pre-trained on two tasks simultaneously:\n",
        "\n",
        "1. MASKED LANGUAGE MODELING (MLM):\n",
        "\n",
        "   Original: \"The cat sat on the mat\"\n",
        "   Masked:   \"The cat [MASK] on the [MASK]\"\n",
        "   Task:     Predict \"sat\" and \"mat\"\n",
        "\n",
        "   - Randomly mask 15% of tokens\n",
        "   - Force bidirectional context usage\n",
        "   - Can't just look left or right\n",
        "\n",
        "2. NEXT SENTENCE PREDICTION (NSP):\n",
        "\n",
        "   Input: [CLS] Sentence A [SEP] Sentence B [SEP]\n",
        "   Task:  Is B the actual next sentence after A?\n",
        "\n",
        "   Positive: \"I love cats. [SEP] They are cute pets.\"   Yes\n",
        "   Negative: \"I love cats. [SEP] The sky is blue.\"      No\n",
        "\n",
        "   - Teaches relationships between sentences\n",
        "   - Important for QA and NLI tasks\n",
        "\n",
        "INPUT REPRESENTATION:\n",
        "=====================\n",
        "\n",
        "Three embeddings are summed:\n",
        "\n",
        "Token Embeddings:    [CLS] [I] [love] [cats] [SEP]\n",
        "Segment Embeddings:  [ EA ] [ EA] [ EA] [ EA] [EA ]\n",
        "Position Embeddings: [ E0 ] [ E1] [ E2] [ E3] [E4 ]\n",
        "                     \n",
        "Final Input Embedding (768-dim vectors)\n",
        "\n",
        "- Token: WordPiece vocabulary (30,000 tokens)\n",
        "- Segment: Which sentence (A or B)\n",
        "- Position: Where in sequence (0 to 512)\n",
        "\n",
        "FINE-TUNING FOR SENTIMENT ANALYSIS:\n",
        "====================================\n",
        "\n",
        "1. Take pre-trained BERT\n",
        "2. Add a classification layer on top of [CLS] token\n",
        "3. Train only the classification layer (freeze BERT) OR\n",
        "4. Fine-tune entire model (update BERT + classifier)\n",
        "\n",
        "[CLS] This movie is great! [SEP]\n",
        "   \n",
        "BERT Layers (12 transformer blocks)\n",
        "   \n",
        "Take [CLS] output (768-dim)\n",
        "   \n",
        "Classification Layer (768  3 classes)\n",
        "   \n",
        "[Negative, Neutral, Positive]\n",
        "\n",
        "WHY [CLS] TOKEN:\n",
        "================\n",
        "- Special token added at the start\n",
        "- Its representation aggregates info from entire sequence\n",
        "- Trained to capture sentence-level meaning\n",
        "- Perfect for classification tasks\n",
        "\n",
        "BERT VARIANTS:\n",
        "==============\n",
        "- RoBERTa: Better pre-training, removes NSP\n",
        "- ALBERT: Parameter sharing, lighter model\n",
        "- DistilBERT: Smaller, faster, 97% performance\n",
        "- ELECTRA: Different pre-training approach\n",
        "\n",
        "MATHEMATICAL DETAILS:\n",
        "=====================\n",
        "\n",
        "Self-Attention Computation:\n",
        "1. Linear projections: Q = XW_Q, K = XW_K, V = XW_V\n",
        "2. Scaled dot-product: scores = QK^T / d_k\n",
        "3. Softmax: attention_weights = softmax(scores)\n",
        "4. Weighted sum: output = attention_weights  V\n",
        "\n",
        "Feed-Forward Network:\n",
        "FFN(x) = max(0, xW + b)W + b\n",
        "Where: 768  3072  768 dimensions\n",
        "\n",
        "Layer Normalization:\n",
        "LN(x) =   (x - ) /  + \n",
        "Normalizes across features, not batch\n",
        "\n",
        "PARAMETERS IN BERT-BASE:\n",
        "========================\n",
        "- Embeddings: 23.4M parameters\n",
        "- 12  Encoder layers: 85M parameters\n",
        "- Total: ~110M parameters\n",
        "- All trained on Wikipedia + Books\n",
        "\n",
        "COMPUTATIONAL REQUIREMENTS:\n",
        "============================\n",
        "- Pre-training: 16 TPU chips  4 days\n",
        "- Fine-tuning: 1 GPU  few hours\n",
        "- Inference: Can run on CPU (slow) or GPU (fast)\n",
        "\n",
        "KEY PAPERS TO READ:\n",
        "===================\n",
        "1. \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
        "   - Original Transformer architecture\n",
        "\n",
        "2. \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
        "   (Devlin et al., 2018)\n",
        "   - BERT paper\n",
        "\n",
        "3. \"Illustrated BERT\" by Jay Alammar\n",
        "   - Visual explanations online\n",
        "\n",
        "------------------\n",
        "We personally liked some others resources of info, and most of the real learning happened here:\n",
        "\n",
        "1. in-depth blog-type article: https://transformer-circuits.pub/2021/framework/index.html\n",
        "2. Some questions to help vizualisation: https://transformer-circuits.pub/2021/exercises/index.html\n",
        "3. This great playlist from 3Blue1Brown: https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=K3CSuV-4xUd-G3Ph (mostrly chapters 5 and 6)\n",
        "\"\"\")\n",
        "\n",
        "# Create BERT architecture visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Architecture diagram (simplified)\n",
        "axes[0].text(0.5, 0.95, 'BERT Architecture', ha='center', fontsize=16, weight='bold')\n",
        "y_pos = 0.85\n",
        "boxes = [\n",
        "    ('Input: [CLS] + Tokens + [SEP]', 'lightblue'),\n",
        "    ('Token + Segment + Position Embeddings', 'lightgreen'),\n",
        "    ('Transformer Layer 1 (Attention + FFN)', 'lightyellow'),\n",
        "    ('...', 'white'),\n",
        "    ('Transformer Layer 12', 'lightyellow'),\n",
        "    ('[CLS] Output (768-dim)', 'lightcoral'),\n",
        "    ('Classification Head', 'lightpink'),\n",
        "    ('Predictions', 'lightgray')\n",
        "]\n",
        "\n",
        "for text, color in boxes:\n",
        "    axes[0].add_patch(plt.Rectangle((0.2, y_pos-0.04), 0.6, 0.07,\n",
        "                                    facecolor=color, edgecolor='black', linewidth=2))\n",
        "    axes[0].text(0.5, y_pos, text, ha='center', va='center', fontsize=10)\n",
        "    y_pos -= 0.11\n",
        "\n",
        "axes[0].set_xlim([0, 1])\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Attention visualization concept\n",
        "axes[1].text(0.5, 0.95, 'Self-Attention Concept', ha='center', fontsize=16, weight='bold')\n",
        "sentence = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
        "y_positions = np.linspace(0.8, 0.2, len(sentence))\n",
        "\n",
        "for i, (word, y) in enumerate(zip(sentence, y_positions)):\n",
        "    axes[1].text(0.3, y, word, fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
        "    axes[1].text(0.7, y, word, fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
        "\n",
        "# Draw attention lines (example: \"cat\" attending to \"sat\")\n",
        "axes[1].plot([0.35, 0.68], [y_positions[1], y_positions[2]], 'r-', linewidth=2, alpha=0.7)\n",
        "axes[1].plot([0.35, 0.68], [y_positions[1], y_positions[0]], 'b-', linewidth=1, alpha=0.3)\n",
        "axes[1].plot([0.35, 0.68], [y_positions[1], y_positions[4]], 'b-', linewidth=1, alpha=0.3)\n",
        "\n",
        "axes[1].text(0.3, 0.92, 'Query', ha='center', fontsize=10, weight='bold')\n",
        "axes[1].text(0.7, 0.92, 'Key/Value', ha='center', fontsize=10, weight='bold')\n",
        "axes[1].text(0.5, 0.05, 'Red line = high attention\\nBlue lines = low attention',\n",
        "             ha='center', fontsize=9)\n",
        "\n",
        "axes[1].set_xlim([0, 1])\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "15pl3aaU6wxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}